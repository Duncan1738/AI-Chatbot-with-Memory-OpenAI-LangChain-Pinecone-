{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udd16 AI Chatbot with Memory (LangChain + OpenAI + Pinecone)\n", "This notebook builds an **AI-powered chatbot** that:\n", "- \u2705 **Uses OpenAI's GPT model for intelligent responses**\n", "- \u2705 **Maintains memory using Pinecone vector storage**\n", "- \u2705 **Retrieves relevant context for dynamic conversations**\n", "- \u2705 **Can be extended with custom knowledge bases**\n", "\n", "---"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc Install Dependencies\n", "!pip install openai langchain pinecone-client dotenv"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd11 Set Up API Keys\n", "You'll need API keys for:\n", "- **OpenAI API** (for GPT responses)\n", "- **Pinecone API** (for memory storage)\n", "\n", "Run the following cell and enter your keys."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "from dotenv import load_dotenv\n", "\n", "# Load API keys from environment variables\n", "load_dotenv()\n", "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n", "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n", "\n", "# Ensure keys are provided\n", "assert OPENAI_API_KEY, \"\u274c OpenAI API key missing! Set OPENAI_API_KEY in your environment.\"\n", "assert PINECONE_API_KEY, \"\u274c Pinecone API key missing! Set PINECONE_API_KEY in your environment.\"\n", "\n", "print(\"\u2705 API keys loaded successfully!\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\udde0 Initialize Pinecone for Chatbot Memory\n", "We'll use **Pinecone** to store and retrieve past chat interactions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pinecone\n", "\n", "# Initialize Pinecone\n", "pinecone.init(api_key=PINECONE_API_KEY, environment=\"us-west1-gcp\")\n", "\n", "# Create a Pinecone index\n", "index_name = \"chatbot-memory\"\n", "if index_name not in pinecone.list_indexes():\n", "    pinecone.create_index(index_name, dimension=1536, metric=\"cosine\")\n", "\n", "# Connect to the index\n", "index = pinecone.Index(index_name)\n", "print(\"\u2705 Pinecone index initialized!\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\udd16 Define Chatbot Function\n", "This function will:\n", "- Retrieve past chat history from Pinecone\n", "- Use OpenAI's GPT model to generate responses\n", "- Store new conversations in Pinecone\n", "- Return a chatbot response"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from langchain.chains import ConversationalRetrievalChain\n", "from langchain.chat_models import ChatOpenAI\n", "\n", "# Initialize OpenAI chat model\n", "llm = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-4\")\n", "\n", "def chatbot(query, user_id):\n", "    \"\"\"Handles chat queries with memory retrieval.\"\"\"\n", "    \n", "    # Retrieve past conversations from Pinecone\n", "    past_chats = index.query(user_id, top_k=3, include_metadata=True)\n", "    chat_history = \"\\n\".join([item[\"metadata\"][\"text\"] for item in past_chats[\"matches\"]])\n", "    \n", "    # Use GPT model to generate response\n", "    chain = ConversationalRetrievalChain(llm=llm, retriever=None)\n", "    response = chain.run(input=query, chat_history=chat_history)\n", "    \n", "    # Store the new interaction in Pinecone\n", "    index.upsert(vectors=[(user_id, response, {\"text\": query})])\n", "    \n", "    return response\n", "\n", "print(\"\u2705 Chatbot function ready!\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcac Chat with AI\n", "Test the chatbot by entering **user messages**."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example conversation\n", "user_id = \"user123\"\n", "\n", "while True:\n", "    user_input = input(\"You: \")\n", "    if user_input.lower() in [\"exit\", \"quit\"]:\n", "        print(\"\ud83d\udc4b Exiting chatbot.\")\n", "        break\n", "    response = chatbot(user_input, user_id)\n", "    print(\"AI:\", response)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83c\udf89 Done!\n", "Your **AI chatbot with memory** is now running! \ud83d\ude80\ud83e\udd16"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 4}